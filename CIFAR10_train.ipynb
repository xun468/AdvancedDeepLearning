{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install adversarial-robustness-toolbox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import os\n",
    "import time\n",
    "\n",
    "import keras\n",
    "\n",
    "from keras.datasets import cifar10\n",
    "\n",
    "from art.attacks import FastGradientMethod, ProjectedGradientDescent\n",
    "from art.classifiers import KerasClassifier\n",
    "from art.utils import load_dataset\n",
    "from art.attacks import BasicIterativeMethod\n",
    "from art.defences import AdversarialTrainer\n",
    "from art.data_generators import KerasDataGenerator\n",
    "\n",
    "from keras.datasets import cifar10\n",
    "from keras.optimizers import adam\n",
    "from keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPool2D,GlobalAveragePooling2D\n",
    "from keras.models import Sequential,Model,load_model\n",
    "from keras import applications\n",
    "\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "\n",
    "from keras.optimizers import SGD, Adam\n",
    "import numpy as np\n",
    "\n",
    "from keras.callbacks import LearningRateScheduler\n",
    "from math import floor\n",
    "from math import ceil\n",
    "\n",
    "import tensorflow as tf\n",
    "tf.get_logger().setLevel('ERROR')\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tensorflow.python.keras.utils.data_utils import Sequence\n",
    "\n",
    "\n",
    "# print(keras.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train, y_train), (x_test, y_test), min_, max_ = load_dataset(str('cifar10'))\n",
    "# (x_train1, y_train1), (x_test1, y_test1) = cifar10.load_data() # difference is only one hot y vector?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST_RUN = False\n",
    "USE_PARTIAL_DATA = False\n",
    "\n",
    "if TEST_RUN:\n",
    "    USE_PARTIAL_DATA = True # how much of data we use for training\n",
    "PARTIAL_SIZE = 256*8\n",
    "img_size=32\n",
    "\n",
    "USE_WEIGHT_DECAY = True\n",
    "\n",
    "SHOW_DATA_AUG = False\n",
    "\n",
    "USE_RETRAINING = False   # Either use retrain or train directly using PGD\n",
    "\n",
    "VERBOSITY = 0  # Keras fit verbosity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class lr_schedule:\n",
    "    def __init__(self, lr_init=0.01, factor=0.1, schedule=(10000, 10000)):\n",
    "        self.schedule = schedule\n",
    "        self.lr_init = lr_init\n",
    "        self.factor = factor\n",
    "    \n",
    "    def __call__(self, epoch):\n",
    "        exp = np.heaviside(epoch-self.schedule[0], 1) + np.heaviside(epoch-self.schedule[1], 1)\n",
    "        return self.lr_init * self.factor ** exp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1960\n"
     ]
    }
   ],
   "source": [
    "callbacks = []\n",
    "\n",
    "epochs = 2\n",
    "\n",
    "if USE_RETRAINING:\n",
    "    epochs = 10\n",
    "    batch_size = 256\n",
    "    momentum = 0.9\n",
    "    weight_decay = 5e-4\n",
    "    \n",
    "    retraining_rounds = 40  # Affects time the most, but probably the most important part\n",
    "    epochs_per_round = 1  # More than 2 here is unnecessary I think\n",
    "    \n",
    "    lr = 0.0001 # lr = 0.01\n",
    "    optimizer = Adam(lr=lr) # keras.optimizers.SGD(lr=lr, momentum=momentum) # Adam(lr=lr)\n",
    "    loss = 'categorical_crossentropy'\n",
    "    metrics = ['accuracy']\n",
    "    \n",
    "else:\n",
    "    # trying to emulate their exact setup\n",
    "    epochs1 = 150\n",
    "    epochs2 = 100\n",
    "    epochs3 = 100\n",
    "    initial_epoch = 0\n",
    "    batch_size = 256\n",
    "    weight_decay = 5e-4\n",
    "    \n",
    "    momentum = 0.9\n",
    "    lr1 = 0.01\n",
    "    lr2 = 0.001\n",
    "    lr3 = 0.0001\n",
    "    # scheduler = lr_schedule(lr_init=lr, factor=0.1, schedule=(20, 35))\n",
    "    \n",
    "    # lr-schedule\n",
    "    # callbacks = [LearningRateScheduler(scheduler)]\n",
    "    \n",
    "    # Data augmentation\n",
    "    datagen = ImageDataGenerator(\n",
    "                horizontal_flip=True,\n",
    "                width_shift_range=0.1,\n",
    "                height_shift_range=0.1,\n",
    "                zoom_range=0.1,\n",
    "                fill_mode='nearest')\n",
    "    train_generator = datagen.flow(x=x_train, y=y_train, batch_size=batch_size)\n",
    "    \n",
    "    optimizer = keras.optimizers.SGD(lr=lr1, momentum=momentum, decay=0.0)\n",
    "    loss = 'categorical_crossentropy'\n",
    "    metrics = ['accuracy']\n",
    "    \n",
    "    adv_ratio = 1.0\n",
    "    \n",
    "#Adv training\n",
    "norm = 2\n",
    "eps = 0.5\n",
    "steps = 7\n",
    "step_size = 0.1\n",
    "targeted = False\n",
    "\n",
    "# MP\n",
    "use_multiprocessing = False\n",
    "\n",
    "# Checkpoints\n",
    "checkpoint_interval = 10\n",
    "checkpoint_interval *= ceil(x_train.shape[0] / batch_size)\n",
    "print(checkpoint_interval)\n",
    "\n",
    "filepath=\"impro-{epoch:02d}-{acc:.2f}.hdf5\"\n",
    "# checkpoint = ModelCheckpoint(filepath, monitor='acc', verbose=1, save_best_only=True, mode='max')\n",
    "checkpoint = ModelCheckpoint(filepath, period=checkpoint_interval, monitor='acc')\n",
    "callbacks.append(checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = x_train.shape[1:]\n",
    "no_classes = y_train.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = applications.resnet50.ResNet50(weights=None, include_top=True, input_shape=input_shape, classes=no_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "if SHOW_DATA_AUG:\n",
    "    test_datagen = ImageDataGenerator(\n",
    "                    # samplewise_std_normalization=True,\n",
    "                    horizontal_flip=True,\n",
    "                    width_shift_range=0.1,\n",
    "                    height_shift_range=0.1,\n",
    "                    zoom_range=0.1,\n",
    "                    fill_mode='constant',\n",
    "                    cval=0.5)\n",
    "    test_gen = test_datagen.flow(x=x_train, y=y_train, batch_size=1)\n",
    "    count = 0\n",
    "    for img in test_gen:\n",
    "        plt.imshow(img[0][0])\n",
    "        plt.show()\n",
    "        count += 1\n",
    "        if count > 10:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "if USE_WEIGHT_DECAY:   # https://jricheimer.github.io/keras/2019/02/06/keras-hack-1/\n",
    "    for layer in model.layers:\n",
    "        if isinstance(layer, keras.layers.Conv2D) or isinstance(layer, keras.layers.Dense):\n",
    "            layer.add_loss(keras.regularizers.l2(weight_decay)(layer.kernel))\n",
    "        if hasattr(layer, 'bias_regularizer') and layer.use_bias:\n",
    "            layer.add_loss(keras.regularizers.l2(weight_decay)(layer.bias))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "if USE_PARTIAL_DATA:\n",
    "    x_train, y_train = x_train[:PARTIAL_SIZE], y_train[:PARTIAL_SIZE]\n",
    "    x_test, y_test = x_test[:PARTIAL_SIZE], y_test[:PARTIAL_SIZE]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=optimizer, loss=loss, metrics=metrics)\n",
    "classifier = KerasClassifier(model=model, clip_values=(min_, max_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "attack = ProjectedGradientDescent(classifier=classifier, norm=norm, eps=eps, eps_step=step_size, max_iter=steps, targeted=targeted, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BEFORE TRAINING:\n",
      "Accuracy on benign test examples: 11.20%\n",
      "Accuracy on adversarial test examples: 13.20%\n"
     ]
    }
   ],
   "source": [
    "test_size = 500\n",
    "print(\"BEFORE TRAINING:\")\n",
    "predictions = classifier.predict(x_test[:test_size])\n",
    "accuracy = np.sum(np.argmax(predictions, axis=1) == np.argmax(y_test[:test_size], axis=1)) / len(y_test[:test_size])\n",
    "print('Accuracy on benign test examples: {:.2f}%'.format(accuracy * 100))\n",
    "\n",
    "x_test_adv = attack.generate(x=x_test[:test_size])\n",
    "predictions = classifier.predict(x_test_adv)\n",
    "accuracy = np.sum(np.argmax(predictions, axis=1) == np.argmax(y_test[:test_size], axis=1)) / len(y_test[:test_size])\n",
    "print('Accuracy on adversarial test examples: {:.2f}%'.format(accuracy * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/jupyter/CIFAR10\n"
     ]
    }
   ],
   "source": [
    "# Saving trained model here\n",
    "path = os.getcwd()\n",
    "print(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## Stage 1 ##\n",
      "## Stage 2 ##\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "## Stage 3 ##\n",
      "Training took 97050s.\n"
     ]
    }
   ],
   "source": [
    "tf.get_logger().setLevel('INFO')\n",
    "if TEST_RUN:\n",
    "    generator = KerasDataGenerator(train_generator, x_train.shape[0], batch_size)\n",
    "    trainer = AdversarialTrainer(classifier, attack, ratio=adv_ratio)\n",
    "    trainer.fit_generator(generator=generator, nb_epochs=epochs, verbose=VERBOSITY, callbacks=callbacks, initial_epoch=initial_epoch, use_multiprocessing=use_multiprocessing)  \n",
    "    \n",
    "elif USE_RETRAINING:\n",
    "    # Initial training\n",
    "    start_time = time.time()\n",
    "    classifier.fit(x_train, y_train, nb_epochs=epochs, batch_size=batch_size, verbose=VERBOSITY)\n",
    "    \n",
    "    file_name = 'robust_cifar10_init.h5'\n",
    "    classifier.save(filename=file_name, path=path)\n",
    "    print(\"Pretraining took {:.2f}s.\".format(time.time() - start_time))\n",
    "    \n",
    "    print(\"MID TRAINING:\")\n",
    "    predictions = classifier.predict(x_test[:test_size])\n",
    "    accuracy = np.sum(np.argmax(predictions, axis=1) == np.argmax(y_test[:test_size], axis=1)) / len(y_test[:test_size])\n",
    "    print('Accuracy on benign test examples: {:.2f}%'.format(accuracy * 100))\n",
    "    predictions = classifier.predict(x_test_adv)\n",
    "    accuracy = np.sum(np.argmax(predictions, axis=1) == np.argmax(y_test[:test_size], axis=1)) / len(y_test[:test_size])\n",
    "    print('Accuracy on adversarial test examples: {:.2f}%'.format(accuracy * 100))\n",
    "    \n",
    "    # Retraining for adversarial examples\n",
    "    start_time_re = time.time()\n",
    "    for i in range(retraining_rounds):\n",
    "        print(\"Retraining round {}/{}\".format(i+1, retraining_rounds))\n",
    "        # Generating adv examples\n",
    "        \n",
    "        start_time = time.time()\n",
    "        attack = ProjectedGradientDescent(classifier=classifier, norm=norm, eps=eps, eps_step=step_size, max_iter=steps, targeted=targeted, batch_size=batch_size)\n",
    "        adv_x_train = attack.generate(x=x_train, y=y_train)\n",
    "        print(\"Generating adversarial examples took {:.2f}s.\".format(time.time() - start_time))\n",
    "        \n",
    "        # Retraining, a few fewer epochs I guess?\n",
    "        classifier.fit(adv_x_train, y_train, nb_epochs=epochs_per_round, batch_size=batch_size, verbose=VERBOSITY)\n",
    "        \n",
    "        file_name = 'robust_cifar10_re' + str(i) + '.h5'\n",
    "        classifier.save(filename=file_name, path=path)\n",
    "    \n",
    "    print(\"Re-training took {:.2f}s.\".format(time.time() - start_time_re))\n",
    "\n",
    "else:\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Wrapper for generator\n",
    "    # Because different learning rates does not seem to work with this wrapper, we do it manually.\n",
    "    \n",
    "    generator = KerasDataGenerator(train_generator, x_train.shape[0], batch_size)\n",
    "    \n",
    "    print(\"## Stage 1 ##\")\n",
    "    # trainer = AdversarialTrainer(classifier, attack, ratio=adv_ratio)\n",
    "    # trainer.fit_generator(generator=generator, nb_epochs=epochs1, verbose=VERBOSITY, callbacks=callbacks, initial_epoch=initial_epoch, use_multiprocessing=use_multiprocessing)  \n",
    "    # classifier.save(filename='stage1.h5', path=path)\n",
    "    \n",
    "    print(\"## Stage 2 ##\")\n",
    "    model2 = load_model('stage1.h5')\n",
    "    model2.compile(optimizer=keras.optimizers.SGD(lr=lr2, momentum=momentum, decay=0.0), loss=loss, metrics=metrics)\n",
    "    classifier2 = KerasClassifier(model=model2, clip_values=(min_, max_))\n",
    "    attack2 = ProjectedGradientDescent(classifier=classifier2, norm=norm, eps=eps, eps_step=step_size, max_iter=steps, targeted=targeted, batch_size=batch_size)\n",
    "    \n",
    "    trainer2 = AdversarialTrainer(classifier2, attack2, ratio=adv_ratio)\n",
    "    trainer2.fit_generator(generator=generator, nb_epochs=epochs2, verbose=VERBOSITY, callbacks=callbacks, initial_epoch=initial_epoch, use_multiprocessing=use_multiprocessing)  \n",
    "    classifier2.save(filename='stage2.h5', path=path)\n",
    "    \n",
    "    print(\"## Stage 3 ##\")\n",
    "    model3 = load_model('stage2.h5')\n",
    "    model3.compile(optimizer=keras.optimizers.SGD(lr=lr3, momentum=momentum, decay=0.0), loss=loss, metrics=metrics)\n",
    "    classifier3 = KerasClassifier(model=model3, clip_values=(min_, max_))\n",
    "    attack3 = ProjectedGradientDescent(classifier=classifier3, norm=norm, eps=eps, eps_step=step_size, max_iter=steps, targeted=targeted, batch_size=batch_size)\n",
    "    \n",
    "    trainer3 = AdversarialTrainer(classifier3, attack3, ratio=adv_ratio)\n",
    "    trainer3.fit_generator(generator=generator, nb_epochs=epochs3, verbose=VERBOSITY, callbacks=callbacks, initial_epoch=initial_epoch, use_multiprocessing=use_multiprocessing)  \n",
    "    \n",
    "    classifier3.save(filename='stage3.h5', path=path)\n",
    "    print(\"Training took {:.0f}s.\".format(time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AFTER TRAINING:\n",
      "Accuracy on benign test examples: 70.80%\n",
      "Accuracy on OLD adversarial test examples: 70.40%\n",
      "Accuracy on NEW adversarial test examples: 70.40%\n"
     ]
    }
   ],
   "source": [
    "test_size = len(y_test)\n",
    "print(\"AFTER TRAINING:\")\n",
    "predictions = classifier3.predict(x_test[:test_size])\n",
    "accuracy = np.sum(np.argmax(predictions, axis=1) == np.argmax(y_test[:test_size], axis=1)) / len(y_test[:test_size])\n",
    "print('Accuracy on benign test examples: {:.2f}%'.format(accuracy * 100))\n",
    "\n",
    "predictions = classifier3.predict(x_test_adv)\n",
    "accuracy = np.sum(np.argmax(predictions, axis=1) == np.argmax(y_test[:test_size], axis=1)) / len(y_test[:test_size])\n",
    "print('Accuracy on OLD adversarial test examples: {:.2f}%'.format(accuracy * 100))\n",
    "\n",
    "attack = ProjectedGradientDescent(classifier=classifier, norm=norm, eps=eps, eps_step=step_size, max_iter=steps, targeted=targeted, batch_size=batch_size)\n",
    "x_test_adv = attack.generate(x=x_test[:test_size])\n",
    "predictions = classifier3.predict(x_test_adv)\n",
    "accuracy = np.sum(np.argmax(predictions, axis=1) == np.argmax(y_test[:test_size], axis=1)) / len(y_test[:test_size])\n",
    "print('Accuracy on NEW adversarial test examples: {:.2f}%'.format(accuracy * 100))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
